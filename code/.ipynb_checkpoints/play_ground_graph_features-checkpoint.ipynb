{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949d5d23",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0bdbf8",
   "metadata": {},
   "source": [
    "## Graph, Nodes and Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b43ac1",
   "metadata": {},
   "source": [
    "In graph theory, a graph is a collection of nodes (also known as vertices) and edges. The nodes represent entities or elements, while the edges represent the connections or relationships between these entities. \n",
    "\n",
    "**Nodes**: Nodes, also referred to as vertices, are the fundamental units in a graph. They represent individual elements or entities within a network. Nodes can represent various things depending on the context of the graph. For example, in a social network graph, nodes can represent individuals, while in a transportation network, nodes can represent cities or intersections.\n",
    "\n",
    "**Edges**: Edges are the connections or links between nodes in a graph. They represent the relationships or interactions between the entities represented by the nodes. Edges can be directed or undirected, depending on whether the relationship between nodes has a specific direction or not. Directed edges have an arrow indicating the direction of the relationship, while undirected edges have no specific direction. Edges in a graph represent relationships or interactions between the nodes. The nature of these edges can be very diverse depending on the context of your data and the problem you are solving. Here are a few examples of how edges can be defined:\n",
    "\n",
    "1. **Social Networks**: If your nodes are individuals, an edge could represent a friendship, a follow, or a connection on a social media platform.\n",
    "\n",
    "2. **Financial Networks**: If your nodes are banks, an edge could represent lending and borrowing relationships. In a financial transaction network, an edge could represent a transaction between two individuals.\n",
    "\n",
    "3. **Communication Networks**: If your nodes are individual email addresses, an edge could represent the exchange of emails between them.\n",
    "\n",
    "4. **Collaboration Networks**: If your nodes are scientists, an edge could represent co-authorship on a paper.\n",
    "\n",
    "5. **Infrastructure Networks**: If your nodes are cities, an edge could represent a direct road or flight connection between them.\n",
    "\n",
    "In your specific case, as you're dealing with a financial dataset (default/no default), the creation of edges depends on the context:\n",
    "\n",
    "- If your data includes transactions, you could create edges between entities that have transactions between them.\n",
    "- If your dataset includes information about co-signers or guarantors on loans, you could create edges between these connected individuals.\n",
    "- If your entities belong to the same group (like same organization or family), you could draw edges between them.\n",
    "- You could define edges based on similarity or proximity in the feature space. For instance, you could use a threshold value on a given feature or use a clustering method to group similar entities and then create edges within each group.\n",
    "\n",
    "In order to create a meaningful graph, it's crucial to have a clear understanding of your data and the relationships you're trying to represent. It's important to choose an edge definition that makes sense for your specific problem and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2641b7",
   "metadata": {},
   "source": [
    "## Centrality measures: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc22ece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T17:43:49.925111Z",
     "start_time": "2023-05-21T17:43:49.905824Z"
    }
   },
   "source": [
    "Centrality measures are techniques used in network analysis to identify the most important nodes in a network. There are several types of centrality measures, such as degree centrality, closeness centrality, betweenness centrality, and eigenvector centrality. Here's a brief explanation of each:\n",
    "\n",
    "1. **Degree Centrality**: It is simply the number of edges connected to a node. In directed networks, we can differentiate between in-degree and out-degree centralities.\n",
    "\n",
    "   For an undirected graph, the degree centrality \\(C_D(v)\\) for a node \\(v\\) is calculated as:\n",
    "\n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\(C_D(v) = \\text{deg}(v)\\)\n",
    "   </div>\n",
    "\n",
    "   where \\(\\text{deg}(v)\\) is the degree of the node \\(v\\), i.e., the number of edges incident upon \\(v\\).\n",
    "\n",
    "   For a directed graph, we can define in-degree centrality and out-degree centrality. The in-degree centrality for a node \\(v\\) is the number of incoming edges to \\(v\\), and the out-degree centrality is the number of outgoing edges from \\(v\\).\n",
    "\n",
    "   Reference: Freeman, L. C. (2002). Centrality in social networks: Conceptual clarification. Social network: critical concepts in sociology. Londres: Routledge, 1, 238-263.\n",
    "\n",
    "2. **Closeness Centrality**: It measures how fast information can spread from a given node to other reachable nodes in the network.\n",
    "\n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\[C(x) = \\frac{1}{\\sum_{y}d(y, x)}\\]\n",
    "   </div>\n",
    "\n",
    "   In this formula, \\(d(y, x)\\) is the shortest-path distance from \\(y\\) to \\(x\\), and the sum is over all nodes \\(y\\) in the same connected component as \\(x\\). The idea is that the more central a node is, the closer it is to all other nodes.\n",
    "\n",
    "   Reference: Freeman, L. C. (2002). Centrality in social networks: Conceptual clarification. Social network: critical concepts in sociology. Londres: Routledge, 1, 238-263.\n",
    "\n",
    "3. **Betweenness Centrality**: It is a measure of a node's centrality in a network equal to the number of shortest paths from all vertices to all others that pass through that node.\n",
    "\n",
    "   The mathematical formula for betweenness centrality \\(C_B(v)\\) for a node \\(v\\) is:\n",
    "\n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\[C_B(v) =\\sum_{s,t \\in V} \\frac{\\sigma(s, t|v)}{\\sigma(s, t)}\\]\n",
    "   </div>\n",
    "\n",
    "   In this formula, \\(V\\) is the set of nodes, \\(\\sigma(s, t)\\) is the total number of shortest paths from node \\(s\\) to node \\(t\\), and \\(\\sigma(s, t|v)\\) is the number of those paths that pass through \\(v\\).\n",
    "\n",
    "   Reference: Freeman, L. C. (1977). A set of measures of centrality based on betweenness. Sociometry, 35-41.\n",
    "\n",
    "4. **Eigenvector Centrality**: A node is considered important if it is connected to other important nodes.\n",
    "\n",
    "   The mathematical formula for eigenvector centrality \\(C_E(v)\\) for a node \\(v\\) is defined as:\n",
    "\n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\[CE(v) = \\frac{1}{\\lambda} \\sum_{t \\in M(v)} C_E(t)\\]\n",
    "   </div>\n",
    "\n",
    "   In this formula, \\(M(v)\\) is the set of the neighbors of \\(v\\), and \\(\\lambda\\) is a constant. In other words, the eigenvector centrality for a node is the sum of the centrality scores of its neighbors, scaled by a constant factor.\n",
    "\n",
    "   The calculation of eigenvector centrality is indeed iterative and based on the centrality of the neighboring nodes. This is because the importance of a node in the network is determined not only by how many connections it has, but also by how important its connections are.\n",
    "\n",
    "   The calculation of eigenvector centrality is typically done through the power iteration method. Here's a simplified explanation of the process:\n",
    "\n",
    "   1. Assign all nodes an initial centrality score. This could be a score of 1 for simplicity.\n",
    "\n",
    "   2. For each node, calculate its new centrality score as the sum of the centrality scores of its neighbors from the previous iteration.\n",
    "\n",
    "   3. Normalize the centrality scores so that their sum is 1. This is to prevent the scores from growing or shrinking exponentially in the next iterations.\n",
    "\n",
    "   4. Repeat steps 2 and 3 until the scores converge, i.e., the scores do not change significantly from one iteration to the next.\n",
    "\n",
    "   The result is a vector of centrality scores that is the principal eigenvector of the adjacency matrix of the graph, hence the name \"eigenvector centrality\".\n",
    "\n",
    "   Eigenvector centrality is a measure of the influence of a node in a network. It assigns relative scores to all nodes in the network based on the concept that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes.\n",
    "\n",
    "   Reference: Bonacich, P. (1987). Power and centrality: A family of measures. American journal of sociology, 92(5), 1170-1182.\n",
    "\n",
    "5. **PageRank**: is an algorithm used by Google Search to rank web pages in their search engine results. It works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites. The mathematical formula for PageRank \\(PR(p)\\) for a page \\(p\\) is:\n",
    "\n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\[PR(p) = (1 - d) + d \\sum_{i \\in M(p)} \\frac{PR(i)}{L(i)}\\]\n",
    "   </div>\n",
    "\n",
    "   In this formula, \\(M(p)\\) is the set of pages that link to \\(p\\), \\(L(i)\\) is the number of outbound links on page \\(i\\), and \\(d\\) is a damping factor, usually set to 0.85. The idea is that a page's PageRank is derived from the PageRanks of the pages that link to it. Each of these contributing pages transfers its PageRank to \\(p\\) proportionally to the number of outbound links it has.\n",
    "\n",
    "   Reference: Lawrence, P. (1999). The pagerank citation ranking: Bringing order to the web. [Link](http://dbpubs.stanford.edu:8090/aux/index-en.html)\n",
    "\n",
    "6. **Katz centrality**: Katz centrality is a measure of centrality in a network that takes into account both the direct and indirect influence of a node's neighbors. It was introduced by Leo Katz in his paper \"A New Status Index Derived from    Sociometric Analysis\" published in 1953.\n",
    "\n",
    "   The Katz centrality of a node \\(i\\) is defined as the sum of the contributions from all its neighbors, weighted by a factor \\(\\beta\\) and the number of paths of length \\(k\\) connecting the neighbors to the node \\(i\\). The formula for Katz centrality can be expressed as:\n",
    "\n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\(C_{\\text{Katz}}(i) = \\sum_{j=1}^{n} \\beta A_{ij} C_{\\text{Katz}}(j) + \\alpha\\)\n",
    "   </div>\n",
    "\n",
    "   where:\n",
    "   - \\(C_{\\text{Katz}}(i)\\) represents the Katz centrality of node \\(i\\).\n",
    "   - \\(A_{ij}\\) denotes the adjacency matrix element, indicating the presence or absence of an edge between nodes \\(i\\) and \\(j\\).\n",
    "   - \\(\\beta\\) is a scaling factor that controls the weight given to indirect paths. Typically, \\(|\\beta| < \\frac{1}{\\lambda_{\\text{max}}}\\), where \\(\\lambda_{\\text{max}}\\) is the largest eigenvalue of the adjacency matrix.\n",
    "   - \\(\\alpha\\) is a constant term representing the node's intrinsic centrality.\n",
    "\n",
    "   The Katz centrality algorithm is iterative. Starting with an initial centrality value for each node, the centrality values are updated iteratively using the above formula until convergence is reached.\n",
    "\n",
    "   Katz centrality is useful for identifying influential nodes in a network, as it considers both direct and indirect connections. Nodes with higher Katz centrality scores are considered more central or influential within the network.\n",
    "\n",
    "   Reference: Katz, L. (1953). A new status index derived from sociometric analysis. Psychometrika, 18(1), 39-43.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6. **HITS Algorithm**: The HITS (Hyperlink-Induced Topic Search) algorithm, also known as hubs and authorities, is a link analysis algorithm that rates webpages, developed by Jon Kleinberg. The idea behind Hubs and Authorities stemmed from a particular insight into the creation of web pages when the Internet was originally forming; that is, certain web pages, known as hubs, served as large directories that were not actually authoritative in the information that they held, but were used as compilations of a broad catalog of information that led users to direct to other authoritative pages.\n",
    "\n",
    "   In the context of HITS, each node in a graph has two scores: an authority score and a hub score.\n",
    "\n",
    "   The Authority Score a(i) of a node i is computed as the sum of the hub scores of each node j that points to i. This can be represented mathematically as:\n",
    "\n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\[\n",
    "   a(i) = \\sum_{j \\in M(i)} h(j)\n",
    "   \\]\n",
    "   </div>\n",
    "\n",
    "   where: M(i) is the set of nodes that point to i and h(j) is the hub score of node j.\n",
    "   The Hub Score h(i) of a node i is computed as the sum of the authority scores of each node j that i points to. This can be represented mathematically as:\n",
    "   \n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\[\n",
    "   h(i) = \\sum_{j \\in N(i)} a(j)\n",
    "   \\]\n",
    "   </div>\n",
    "\n",
    "   where N(i) is the set of nodes that i points to and a(j) is the authority score of node j.\n",
    "\n",
    "   The authority and hub scores are calculated iteratively until they converge.\n",
    "\n",
    "   The HITS algorithm was first proposed by Jon Kleinberg in his work:\n",
    "\n",
    "   Reference: Kleinberg, J. M. (1999). Authoritative sources in a hyperlinked environment. Journal of the ACM (JACM), 46(5), 604-632."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc3f68",
   "metadata": {},
   "source": [
    "## Application in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f49957",
   "metadata": {},
   "source": [
    "Now, if you're looking to compute centrality measures using features as inputs, you could consider the features as attributes of the nodes or edges in your network. For instance, if you're predicting default/non-default, each node could be an individual (or entity), and the features could be attributes of those individuals. The edges could represent some relationship between them.\n",
    "\n",
    "However, the computation of centrality doesn't typically involve the use of predictive features as inputs, but rather the structure of the network itself (who is connected to whom). If you have some prediction task related to the nodes of the network, the centrality measures can be used as features to help predict that task.\n",
    "\n",
    "For example, if you are trying to predict default/non-default for individuals in a financial network, you could calculate the centrality measures for each individual (node) in the network. These centrality measures could then serve as input features to a machine learning model (along with any other features you have about the individuals) to predict default/non-default.\n",
    "\n",
    "Here is a general way you might approach this using Python's NetworkX library:\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "\n",
    "# Create a graph object\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges to your graph using your data\n",
    "# You can also add attributes to nodes and edges if you have additional features\n",
    "# For example:\n",
    "# G.add_node(node_id, attr_dict={feature1:value1, feature2:value2,...})\n",
    "# G.add_edge(node1_id, node2_id, attr_dict={feature1:value1, feature2:value2,...})\n",
    "\n",
    "# Calculate centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "page_rank = nx.pagerank(G, alpha=0.85)  # alpha is the damping parameter\n",
    "katz_centrality = nx.katz_centrality(G, alpha=0.1, beta=1.0)\n",
    "hubs, authorities = nx.hits(G)\n",
    "\n",
    "# You can then add these as node attributes (which could serve as additional features for your prediction task)\n",
    "for node_id in G.nodes():\n",
    "    G.nodes[node_id]['degree_centrality'] = degree_centrality[node_id]\n",
    "    G.nodes[node_id]['closeness_centrality'] = closeness_centrality[node_id]\n",
    "    G.nodes[node_id]['betweenness_centrality'] = betweenness_centrality[node_id]\n",
    "    G.nodes[node_id]['eigenvector_centrality'] = eigenvector_centrality[node_id]\n",
    "```\n",
    "\n",
    "These features can then be fed into a machine learning model to predict default/non-default. The exact method of doing this will depend on the specific machine learning model you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee18b61",
   "metadata": {},
   "source": [
    "## Edges based on feature similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c0c61",
   "metadata": {},
   "source": [
    "Defining edges based on similarity or proximity in the feature space is a common technique used in the field of network science. Here is a basic example of how to create edges based on Euclidean distance, a common measure of proximity in the feature space:\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_classification(n_samples=100, n_features=5, n_informative=2, n_redundant=0, random_state=1)\n",
    "\n",
    "# Convert feature matrix into a DataFrame\n",
    "df = pd.DataFrame(X, columns=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5'])\n",
    "df['Default'] = y\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "for index, row in df.iterrows():\n",
    "    G.add_node(index, attr_dict=row.to_dict())\n",
    "\n",
    "# Compute pairwise Euclidean distances\n",
    "distances = euclidean_distances(df.values)\n",
    "\n",
    "# Define a threshold to decide if an edge should be added\n",
    "threshold = distances.mean()\n",
    "\n",
    "# Add edges between nodes that are closer than the threshold distance\n",
    "for i in range(distances.shape[0]):\n",
    "    for j in range(i+1, distances.shape[0]):  # we only need to look at half the matrix\n",
    "        if distances[i, j] < threshold:\n",
    "            G.add_edge(i, j)\n",
    "```\n",
    "\n",
    "This will create a graph where each node represents an individual, and an edge exists between any two individuals if the Euclidean distance between their feature vectors is less than the average Euclidean distance across all pairs of individuals.\n",
    "\n",
    "Remember that the choice of distance metric (Euclidean in this case) and threshold is crucial and can greatly affect the resulting graph. You may want to experiment with different distance metrics (e.g., Manhattan, Cosine, etc.) and thresholds to find what works best for your specific case.\n",
    "\n",
    "Also, this approach does not scale well for large datasets due to the computation of the pairwise distance matrix. For large datasets, consider using more scalable methods like Nearest Neighbors or approximate methods for large-scale similarity computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec3e5f",
   "metadata": {},
   "source": [
    "## Edges for a weighted graph based on feature similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8127e2",
   "metadata": {},
   "source": [
    "If you want to use a weighted graph, you can add weights to the edges that correspond to the similarity or proximity in the feature space. In the context of the previous example, you could use the inverse of the Euclidean distance as the edge weight. This means that nodes that are more similar (smaller distance) will have a stronger connection (larger weight).\n",
    "\n",
    "Here's how you would modify the previous example to create a weighted graph:\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_classification(n_samples=100, n_features=5, n_informative=2, n_redundant=0, random_state=1)\n",
    "\n",
    "# Convert feature matrix into a DataFrame\n",
    "df = pd.DataFrame(X, columns=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5'])\n",
    "df['Default'] = y\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "for index, row in df.iterrows():\n",
    "    G.add_node(index, attr_dict=row.to_dict())\n",
    "\n",
    "# Compute pairwise Euclidean distances\n",
    "distances = euclidean_distances(df.values)\n",
    "\n",
    "# Define a threshold to decide if an edge should be added\n",
    "threshold = distances.mean()\n",
    "\n",
    "# Add edges between nodes that are closer than the threshold distance\n",
    "# Use the inverse distance as the edge weight\n",
    "for i in range(distances.shape[0]):\n",
    "    for j in range(i+1, distances.shape[0]):  # we only need to look at half the matrix\n",
    "        if distances[i, j] < threshold:\n",
    "            weight = 1.0 / distances[i, j]  # inverse distance as weight\n",
    "            G.add_edge(i, j, weight=weight)\n",
    "```\n",
    "\n",
    "In this example, the edge weight is the inverse of the Euclidean distance, so nodes that are closer in the feature space have a higher weight. You can use any function of the distance as the weight depending on your specific requirements. Be careful with the possibility of division by zero when using the inverse distance as weight, you may want to add a small constant in the denominator to avoid this situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a496c0f",
   "metadata": {},
   "source": [
    "## From edge weight to centrality measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb7475b",
   "metadata": {},
   "source": [
    "Now that we have a graph, we can compute several centrality measures. Here's how you can compute degree, closeness, betweenness, and eigenvector centrality for the weighted graph. Note that the interpretation of these measures changes slightly when dealing with weighted graphs.\n",
    "\n",
    "```python\n",
    "# Compute centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G, distance='weight')\n",
    "betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G, weight='weight')\n",
    "\n",
    "# Add these as node attributes\n",
    "for node_id in G.nodes():\n",
    "    G.nodes[node_id]['degree_centrality'] = degree_centrality[node_id]\n",
    "    G.nodes[node_id]['closeness_centrality'] = closeness_centrality[node_id]\n",
    "    G.nodes[node_id]['betweenness_centrality'] = betweenness_centrality[node_id]\n",
    "    G.nodes[node_id]['eigenvector_centrality'] = eigenvector_centrality[node_id]\n",
    "```\n",
    "\n",
    "Here's a brief explanation of each of these measures:\n",
    "\n",
    "1. **Degree Centrality**: In a weighted graph, degree centrality is still calculated as the number of edges connected to a node. This means that nodes with more connections will have a higher degree centrality. However, it doesn't take into account the weight of the edges, meaning that the strength of the connections is not considered in this measure.\n",
    "\n",
    "2. **Closeness Centrality**: This measure calculates the reciprocal of the sum of the shortest path distances from a node to all other nodes in the graph. So, a higher value of closeness centrality implies that a node is more central. In a weighted graph, the 'distances' considered are the weights of the edges, so a node that has stronger connections to all other nodes (i.e., higher edge weights) will have higher closeness centrality.\n",
    "\n",
    "3. **Betweenness Centrality**: This is a measure of the extent to which a node lies on paths between other nodes. Nodes with high betweenness centrality have a large influence on the transfer of items through the network, under the assumption that item transfer follows the shortest paths. When weights are considered, shorter paths are those with higher total weights, so nodes that lie on the paths with the strongest connections will have higher betweenness centrality.\n",
    "\n",
    "4. **Eigenvector Centrality**: This is a measure of the influence of a node in a network. It assigns relative scores to all nodes in the network based on the principle that connections to nodes with high score contribute more to the score of the node in question. In a weighted graph, connections to nodes with high scores and high edge weights contribute more to the score of the node.\n",
    "\n",
    "Remember that each of these measures captures a different aspect of a node's centrality, and the appropriate measure to use depends on the specifics of your problem and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65635f78",
   "metadata": {},
   "source": [
    "## Centrality for binary targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e1cd2f",
   "metadata": {},
   "source": [
    "With your data, which includes a binary target (default or no default) and many numerical features, you can explore several analytical routes.\n",
    "\n",
    "The centrality measures we've computed can be used to examine the relationships between the nodes (which represent your entities - individuals or organizations) in the context of the target variable (default or no default). For instance, you might find that entities with high degree centrality are more (or less) likely to default, indicating a potential relationship between the entity's position in the network and their default risk.\n",
    "\n",
    "Here's how you can explore these relationships:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert node attributes to a DataFrame\n",
    "df_node_attributes = pd.DataFrame(dict(G.nodes(data=True))).T\n",
    "\n",
    "# Plot centrality measures against the default status\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "# Degree centrality\n",
    "axs[0, 0].scatter(df_node_attributes['Default'], df_node_attributes['degree_centrality'])\n",
    "axs[0, 0].set_xlabel('Default')\n",
    "axs[0, 0].set_ylabel('Degree Centrality')\n",
    "\n",
    "# Closeness centrality\n",
    "axs[0, 1].scatter(df_node_attributes['Default'], df_node_attributes['closeness_centrality'])\n",
    "axs[0, 1].set_xlabel('Default')\n",
    "axs[0, 1].set_ylabel('Closeness Centrality')\n",
    "\n",
    "# Betweenness centrality\n",
    "axs[1, 0].scatter(df_node_attributes['Default'], df_node_attributes['betweenness_centrality'])\n",
    "axs[1, 0].set_xlabel('Default')\n",
    "axs[1, 0].set_ylabel('Betweenness Centrality')\n",
    "\n",
    "# Eigenvector centrality\n",
    "axs[1, 1].scatter(df_node_attributes['Default'], df_node_attributes['eigenvector_centrality'])\n",
    "axs[1, 1].set_xlabel('Default')\n",
    "axs[1, 1].set_ylabel('Eigenvector Centrality')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "You can also calculate correlations between these centrality measures and the target variable to see if there are any strong relationships.\n",
    "\n",
    "```python\n",
    "# Calculate correlations\n",
    "correlations = df_node_attributes[['Default', 'degree_centrality', 'closeness_centrality', 'betweenness_centrality', 'eigenvector_centrality']].astype(float).corr()\n",
    "\n",
    "# Display correlations with the target variable\n",
    "print(correlations['Default'])\n",
    "```\n",
    "\n",
    "It's important to note that this is exploratory analysis and correlation does not imply causation. Further investigation would be needed to understand any potential causal relationships.\n",
    "\n",
    "In the long run, these network-based features can also be used in conjunction with your other numerical features to build a predictive model. By including the network features, the model may be able to capture complex relationships that are not captured by the numerical features alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3eb633",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8fa2bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ced57",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.453Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_classification(n_samples=2000, n_features=5, n_informative=2, n_redundant=0, random_state=1)\n",
    "\n",
    "# Automatically name the columns based on the number of features\n",
    "column_names = [f\"Feature{i}\" for i in range(1, X.shape[1])]\n",
    "column_names.append(\"Default\")\n",
    "# Convert feature matrix into a DataFrame\n",
    "df = pd.DataFrame(X, columns=column_names)\n",
    "\n",
    "df['Default'] = y\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50728307",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b412c75",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.454Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features (excluding the 'Default' column)\n",
    "scaler.fit(df.drop('Default', axis=1))\n",
    "\n",
    "# Transform the features\n",
    "scaled_features = scaler.transform(df.drop('Default', axis=1))\n",
    "\n",
    "# Create a new DataFrame for the scaled features\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=column_names[:-1])  # Exclude the 'Default' column name\n",
    "\n",
    "# Add the 'Default' column back into the DataFrame\n",
    "scaled_df['Default'] = df['Default']\n",
    "\n",
    "print(scaled_df)\n",
    "df = scaled_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7983449",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.455Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def plot_feature_distributions(df_scaled, target_col, file_name = 'feature_distributions.pdf'):\n",
    "    with PdfPages(file_name) as pdf:\n",
    "        # For each feature\n",
    "        for col in df_scaled.columns:\n",
    "            # Exclude the target column\n",
    "            if col != target_col:\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                \n",
    "                # Plot the distribution of this feature for each class\n",
    "                sns.kdeplot(data=df_scaled, x=col, hue=target_col, fill=True)\n",
    "                \n",
    "                plt.title(f'Distribution of {col} by Class')\n",
    "                \n",
    "                # Save the current figure to the pdf\n",
    "                pdf.savefig()\n",
    "                plt.clf()\n",
    "\n",
    "\n",
    "# Call the function\n",
    "plot_feature_distributions(scaled_df, 'Default')\n",
    "import webbrowser\n",
    "\n",
    "webbrowser.open_new(r'feature_distributions.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b8489",
   "metadata": {},
   "source": [
    "### Remove highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e37a7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.456Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_highly_correlated_features(df, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Remove highly correlated features from a DataFrame based on a given threshold.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame.\n",
    "    threshold (float): The threshold for high correlation. Default is 0.8.\n",
    "\n",
    "    Returns:\n",
    "    df_filtered (pandas.DataFrame): The filtered DataFrame with highly correlated features removed.\n",
    "    \"\"\"\n",
    "\n",
    "    features_to_remove = []\n",
    "\n",
    "    # Exclude the \"Default\" column from correlation analysis\n",
    "    df_subset = df.drop(\"Default\", axis=1)\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = df_subset.corr()\n",
    "\n",
    "    # Identify highly correlated features\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                # Append the feature to the removal list\n",
    "                features_to_remove.append(correlation_matrix.columns[j])\n",
    "\n",
    "    # Remove the highly correlated features\n",
    "    df_filtered = df.drop(features_to_remove, axis=1)\n",
    "\n",
    "    return df_filtered, correlation_matrix, features_to_remove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b8c95",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.457Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove highly correlated features\n",
    "df, correlation_matrix,features_to_remove = remove_highly_correlated_features(df, threshold=0.9)\n",
    "print(correlation_matrix)\n",
    "# Print the filtered DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776b018",
   "metadata": {},
   "source": [
    "### Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be969549",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.458Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "\n",
    "# Add nodes to the graph\n",
    "for index, row in df.iterrows():\n",
    "    G.add_node(index, attr_dict=row.to_dict())\n",
    "\n",
    "# Compute pairwise Euclidean distances\n",
    "distances = euclidean_distances(df.values)\n",
    "\n",
    "# Define a threshold to decide if an edge should be added\n",
    "threshold = distances.mean()\n",
    "\n",
    "# Add edges between nodes that are closer than the threshold distance\n",
    "# Use the inverse distance as the edge weight\n",
    "for i in range(distances.shape[0]):\n",
    "    for j in range(i+1, distances.shape[0]):  # we only need to look at half the matrix\n",
    "        if distances[i, j] < threshold:\n",
    "            weight = 1.0 / distances[i, j]  # inverse distance as weight\n",
    "            G.add_edge(i, j, weight=weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7134d1d",
   "metadata": {},
   "source": [
    "### Visualize graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69501dc3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate positions for all nodes in the graph G\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Create a color map\n",
    "color_map = []\n",
    "for node in G:\n",
    "    if G.nodes[node]['attr_dict']['Default'] == 1:\n",
    "        color_map.append('red')\n",
    "    else: \n",
    "        color_map.append('blue')\n",
    "\n",
    "# Draw the graph with node colors\n",
    "nx.draw_networkx_nodes(G, pos, node_color=color_map)\n",
    "nx.draw_networkx_edges(G, pos, edge_color='grey')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308fb308",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.460Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We need pygraphviz for that.\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pygraphviz as pgv\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "# Convert the NetworkX graph G to a PyGraphviz graph\n",
    "A = nx.nx_agraph.to_agraph(G)\n",
    "\n",
    "# Create a layout for the graph\n",
    "pos = graphviz_layout(G, prog='dot')\n",
    "\n",
    "# Draw nodes with color coding for the 'Default' attribute\n",
    "nx.draw_networkx_nodes(G, pos, node_color=color_map)\n",
    "nx.draw_networkx_edges(G, pos, edge_color='grey')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36abd888",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f99053",
   "metadata": {},
   "source": [
    "There are several ways you could visualize this dataset. Because it has 5 numerical features, you could use pairwise scatterplots to visualize the relationship between pairs of features. You could also use histograms or boxplots to visualize the distribution of each feature. Here are examples of how to create these visualizations using matplotlib and seaborn:\n",
    "\n",
    "1. **Pairwise Scatterplots:**\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "\n",
    "# Pairplot of the dataset\n",
    "sns.pairplot(df, hue='Default')\n",
    "```\n",
    "The above code will generate a pairwise scatterplot matrix. Each plot represents the relationship between two features, and data points are colored based on their 'Default' status. The diagonal line of the matrix shows the distribution of the single feature according to the 'Default' categories.\n",
    "\n",
    "2. **Boxplots:**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Boxplots for each feature\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for i, column in enumerate(df.columns[:-1]):  # excluding the 'Default' column\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.boxplot(x='Default', y=column, data=df)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "The boxplots visualize the distribution of each feature for each 'Default' status separately. You can observe the median, quartiles and possible outliers for each feature in each 'Default' category.\n",
    "\n",
    "Remember, these are just simple ways to visualize your dataset and there are many other visualization techniques that can provide deeper insights depending on the nature of your data. For example, a correlation heatmap could be useful to identify highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b641a3b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.462Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Pairplot of the dataset\n",
    "sns.pairplot(df, hue='Default')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88132f6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.463Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_boxplots(df):\n",
    "    \"\"\"\n",
    "    Create boxplots for each feature in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Determine the number of features and calculate the appropriate number of subplots\n",
    "    num_features = len(df.columns) - 1  # excluding the 'Default' column\n",
    "    num_rows = (num_features - 1) // 3 + 1\n",
    "    num_cols = min(num_features, 3)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 10))\n",
    "\n",
    "    # Flatten the axes array if needed\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot boxplots for each feature\n",
    "    for i, column in enumerate(df.columns[:-1]):  # excluding the 'Default' column\n",
    "        ax = axes[i]\n",
    "        sns.boxplot(x='Default', y=column, data=df, ax=ax)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    if num_features < len(axes):\n",
    "        for j in range(num_features, len(axes)):\n",
    "            axes[j].remove()\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to create boxplots for each feature\n",
    "create_boxplots(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534936ed",
   "metadata": {},
   "source": [
    "### Compute minimum spanning tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26e7dbb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.464Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "mst = nx.minimum_spanning_tree(G)\n",
    "import networkx as nx\n",
    "\n",
    "# Create an edge filter for the MST\n",
    "def edge_filter(u, v):\n",
    "    return (u, v) in mst.edges()\n",
    "\n",
    "# Create a filtered subgraph based on the edge filter\n",
    "filtered_graph = G.edge_subgraph((u, v) for u, v in G.edges() if edge_filter(u, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6d5d8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.465Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the filtered subgraph\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True)\n",
    "labels = nx.get_edge_attributes(G, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ed2e7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.466Z"
    }
   },
   "outputs": [],
   "source": [
    "nx.draw_networkx_edges(filtered_graph, pos, edge_color='r', width=2)  # Highlight filtered edges in red\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e3348",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate positions for all nodes in the graph G\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Create a color map\n",
    "color_map = []\n",
    "for node in G:\n",
    "    if G.nodes[node]['attr_dict']['Default'] == 1:\n",
    "        color_map.append('red')\n",
    "    else: \n",
    "        color_map.append('blue')\n",
    "\n",
    "# Draw the graph with node colors\n",
    "nx.draw_networkx_nodes(G, pos, node_color=color_map)\n",
    "nx.draw_networkx_edges(G, pos, edge_color='grey')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3458f7fb",
   "metadata": {},
   "source": [
    "### Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ef9e9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.468Z"
    }
   },
   "outputs": [],
   "source": [
    "nx.betweenness_centrality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036f1c5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.469Z"
    }
   },
   "outputs": [],
   "source": [
    "G = filtered_graph\n",
    "# Compute centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G, distance='weight')\n",
    "betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G, weight='weight')\n",
    "\n",
    "   \n",
    "    \n",
    "# Compute more centrality measures\n",
    "pagerank = nx.pagerank(G, weight='weight')\n",
    "# HITS algorithm returns two dictionaries keyed by node containing hub scores and authority scores\n",
    "hubs, authorities = nx.hits(G, max_iter=1000)\n",
    "\n",
    "# Compute other network measures\n",
    "#avg_shortest_path_length = nx.average_shortest_path_length(G)\n",
    "#density = nx.density(G)\n",
    "#num_connected_components = nx.number_connected_components(G)\n",
    "#avg_clustering_coefficient = nx.average_clustering(G)\n",
    "\n",
    "# Add these as node attributes\n",
    "for node_id in G.nodes():\n",
    "    G.nodes[node_id]['degree_centrality'] = degree_centrality[node_id]\n",
    "    G.nodes[node_id]['closeness_centrality'] = closeness_centrality[node_id]\n",
    "    G.nodes[node_id]['betweenness_centrality'] = betweenness_centrality[node_id]\n",
    "    G.nodes[node_id]['eigenvector_centrality'] = eigenvector_centrality[node_id]\n",
    "\n",
    "    G.nodes[node_id]['pagerank'] = pagerank[node_id]\n",
    "    G.nodes[node_id]['hub_score'] = hubs[node_id]\n",
    "    G.nodes[node_id]['authority_score'] = authorities[node_id]\n",
    "#    G.nodes[node_id]['average_shortest_path_length'] = avg_shortest_path_length\n",
    "#    G.nodes[node_id]['density'] = density\n",
    "#    G.nodes[node_id]['num_connected_components'] = num_connected_components\n",
    "#    G.nodes[node_id]['average_clustering_coefficient'] = avg_clustering_coefficient\n",
    "\n",
    "# Note: Some measures like average shortest path length, density, number of connected components, \n",
    "# and average clustering coefficient are properties of the network as a whole rather than individual nodes. \n",
    "# Therefore, they will be the same for all nodes in a connected graph. \n",
    "# You might want to use these measures for comparison across different graphs or subgraphs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c38d0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert node attributes to a DataFrame and flatten the column structure\n",
    "df_node_attributes = pd.json_normalize(list(dict(G.nodes(data=True)).values()))\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = df_node_attributes.astype(float).corr()\n",
    "\n",
    "# Display correlations with the target variable sorted in descending order\n",
    "print(correlations['attr_dict.Default'].sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e61726",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.471Z"
    }
   },
   "outputs": [],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe77adbe",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.472Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Convert node attributes to a DataFrame and flatten the column structure\n",
    "df_node_attributes = pd.json_normalize(list(dict(G.nodes(data=True)).values()))\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = df_node_attributes.astype(float).corr()\n",
    "\n",
    "# Use a mask to get the lower triangle as the correlation matrix is symmetric\n",
    "mask = np.triu(np.ones_like(correlations, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(correlations, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a30a23",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.473Z"
    }
   },
   "outputs": [],
   "source": [
    "# Call the function\n",
    "import webbrowser\n",
    "plot_feature_distributions(\n",
    "    df_node_attributes,\n",
    "    'attr_dict.Default',\n",
    "    file_name=\"feature_distributions_with_centrality.pdf\")\n",
    "\n",
    "webbrowser.open_new(r'feature_distributions_with_centrality.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b3090",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.474Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_centrality_measures(df_node_attributes):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "    # Degree centrality\n",
    "    axs[0, 0].scatter(df_node_attributes['attr_dict.Default'], df_node_attributes['degree_centrality'])\n",
    "    axs[0, 0].set_xlabel('attr_dict.Default')\n",
    "    axs[0, 0].set_ylabel('Degree Centrality')\n",
    "\n",
    "    # Closeness centrality\n",
    "    axs[0, 1].scatter(df_node_attributes['attr_dict.Default'], df_node_attributes['closeness_centrality'])\n",
    "    axs[0, 1].set_xlabel('attr_dict.Default')\n",
    "    axs[0, 1].set_ylabel('Closeness Centrality')\n",
    "\n",
    "    # Betweenness centrality\n",
    "    axs[1, 0].scatter(df_node_attributes['attr_dict.Default'], df_node_attributes['betweenness_centrality'])\n",
    "    axs[1, 0].set_xlabel('attr_dict.Default')\n",
    "    axs[1, 0].set_ylabel('Betweenness Centrality')\n",
    "\n",
    "    # Eigenvector centrality\n",
    "    axs[1, 1].scatter(df_node_attributes['attr_dict.Default'], df_node_attributes['eigenvector_centrality'])\n",
    "    axs[1, 1].set_xlabel('attr_dict.Default')\n",
    "    axs[1, 1].set_ylabel('Eigenvector Centrality')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce22cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.476Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_centrality_measures(df_node_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da1f6b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.477Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Jupyter notebook\n",
    "os.system(\"jupyter nbconvert 0.1_data_preprocessing.ipynb --to slides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cd70b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.478Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# HTMLPDF\n",
    "os.system(\"pandoc 0.1_data_preprocessing.slides.html -t beamer -o your_notebook.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85438d6a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.479Z"
    }
   },
   "outputs": [],
   "source": [
    "!brew install pandoc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a76869",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T20:28:43.189351Z",
     "start_time": "2023-05-19T20:28:42.900506Z"
    }
   },
   "source": [
    "## Predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be46484",
   "metadata": {},
   "source": [
    "Now that we have our dataset augmented with centrality measures, we can use it to train a predictive model. Let's use a simple logistic regression model from scikit-learn as an example.\n",
    "\n",
    "Firstly, you should split your dataset into training and testing sets. This is a common practice in machine learning to evaluate how well your model can generalize to unseen data.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_node_attributes.drop('Default', axis=1), df_node_attributes['Default'], test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "Now, let's train a logistic regression model on the training set:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Now that the model is trained, you can use it to predict the 'Default' status on the test set:\n",
    "\n",
    "```python\n",
    "# Predict the 'Default' status on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "Finally, we can evaluate the performance of the model by computing metrics like accuracy, precision, recall and the F1 score:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Compute evaluation metrics\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1 score: ', f1_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "This will give you a basic understanding of the performance of your model. Note that logistic regression is a relatively simple model, and depending on your dataset, you might achieve better performance with more complex models, such as Random Forests, Gradient Boosting Machines or Neural Networks. Also, always consider performing model validation (e.g., k-fold cross-validation) and hyperparameter tuning for a more reliable and better performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32331d5a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.480Z"
    }
   },
   "outputs": [],
   "source": [
    "df_node_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e8b81",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.481Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_node_attributes.drop('attr_dict.Default', axis=1),\n",
    "    df_node_attributes['attr_dict.Default'],\n",
    "    test_size=0.2,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa3333",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.482Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a55abc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.483Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27441fe1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.484Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the 'Default' status on the test set\n",
    "y_pred_train = model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73b978",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.485Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Compute evaluation metrics\n",
    "print('Accuracy Train: ', accuracy_score(y_train, y_pred_train))\n",
    "print('Precision Train: ', precision_score(y_train, y_pred_train))\n",
    "print('Recall Train: ', recall_score(y_train, y_pred_train))\n",
    "print('F1 score Train: ', f1_score(y_train, y_pred_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa9e76c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.486Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the 'Default' status on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53de5b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.487Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import evaluation metrics from sklearn.metrics\n",
    "# Accuracy measures the proportion of correct predictions out of total predictions\n",
    "# Precision measures the proportion of true positive predictions out of total positive predictions\n",
    "# Recall (also known as sensitivity) measures the proportion of true positive predictions out of total actual positives\n",
    "# F1 score is the harmonic mean of precision and recall, a balanced measure when classes are imbalanced\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Compute and print accuracy of the model on test data\n",
    "# This tells us the ratio of correctly predicted observations to the total observations\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Compute and print precision of the model on test data\n",
    "# This tells us the ratio of correctly predicted positive observations to the total predicted positives\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "\n",
    "# Compute and print recall of the model on test data\n",
    "# This tells us the ratio of correctly predicted positive observations to the all observations in actual class\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "\n",
    "# Compute and print the F1 score of the model on test data\n",
    "# The F1 score is the weighted average of Precision and Recall, used when we want to seek a balance between Precision and Recall\n",
    "print('F1 score: ', f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da5153",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get feature names and corresponding coefficients\n",
    "feature_names = X_train.columns\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Create a DataFrame for easy visualization\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Calculate the absolute values of the coefficients as a separate column\n",
    "coef_df['AbsCoefficient'] = np.abs(coef_df['Coefficient'])\n",
    "\n",
    "# Sort by absolute coefficient value in descending order\n",
    "coef_df = coef_df.sort_values('AbsCoefficient', ascending=False)\n",
    "\n",
    "print(coef_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a97f9c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.489Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix: \\n', cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34189422",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.490Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc_roc = roc_auc_score(y_test, y_pred)\n",
    "print('AUC-ROC: ', auc_roc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8892e54",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.491Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print('Classification Report: \\n', report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284fc0b5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.491Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print('Cross Validation Score: ', np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc35bf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.492Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(model, X_test, y_test)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957868e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.493Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "plot_roc_curve(model, X_test, y_test)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519491e7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.494Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "plot_precision_recall_curve(model, X_test, y_test)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c48083b",
   "metadata": {},
   "source": [
    "### Grid search and CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be5f6e",
   "metadata": {},
   "source": [
    "Implement more complex models and apply validation techniques using scikit-learn for a Random Forest and a Gradient Boosting model. \n",
    "\n",
    "To make things more robust, I'll include a simple hyperparameter tuning using GridSearchCV, and model validation using k-fold cross-validation:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize the models\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Create a list of models\n",
    "models = [rf, gb]\n",
    "\n",
    "# Define the grid of hyperparameters 'params'\n",
    "params = [\n",
    "    {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},  # RandomForest\n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}   # GradientBoosting\n",
    "]\n",
    "\n",
    "for model, param in zip(models, params):\n",
    "    # GridSearchCV\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param, cv=5)  # 5-fold cross-validation\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(grid.best_params_)  # print the best set of parameters found by GridSearch\n",
    "\n",
    "    # Predict the 'Default' status on the test set\n",
    "    y_pred = grid.predict(X_test)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "```\n",
    "\n",
    "Please note, running this code may take a while, because it tries all combinations of the provided hyperparameters. Also, keep in mind that GridSearchCV applies cross-validation for model validation.\n",
    "\n",
    "As for Neural Networks, it's a bit more involved and typically requires more tuning and computational resources. scikit-learn does offer a simple `MLPClassifier` for multilayer perceptron (MLP) networks, but for more complex architectures, you'll want to look into deep learning libraries like TensorFlow or PyTorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701185e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.495Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize the models\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Create a list of models\n",
    "models = [rf, gb]\n",
    "\n",
    "# Define the grid of hyperparameters 'params'\n",
    "params = [\n",
    "    {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},  # RandomForest\n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}   # GradientBoosting\n",
    "]\n",
    "\n",
    "for model, param in zip(models, params):\n",
    "    # GridSearchCV\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param, cv=5)  # 5-fold cross-validation\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(grid.best_params_)  # print the best set of parameters found by GridSearch\n",
    "\n",
    "    # Predict the 'Default' status on the test set\n",
    "    y_pred = grid.predict(X_test)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e86bdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T21:07:03.319342Z",
     "start_time": "2023-05-19T21:07:03.298465Z"
    }
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef4d6b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.496Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict the 'Default' status on the test set\n",
    "y_pred = grid.best_estimator_.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b42bc1",
   "metadata": {},
   "source": [
    "### Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988f706",
   "metadata": {},
   "source": [
    "Feature importances should be calculated based on the training data. This is because the training data is what the model learns from, and feature importances are a measure of how much each feature contributes to the model's predictions.\n",
    "\n",
    "After training your model, you would typically look at the feature importances to understand which features the model considers important. You can use this information to gain insights into your model and your data. For example, you may find that some features are not important and could be removed, or that some features are very important and perhaps you want to spend more time engineering related features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36613814",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.497Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Compute permutation feature importance\n",
    "result = permutation_importance(grid.best_estimator_, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "# Create a DataFrame to visualize importance scores\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_test.columns,\n",
    "    'Permutation Importance': result.importances_mean,\n",
    "    'Std': result.importances_std\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('Permutation Importance', ascending=False)\n",
    "\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ecf6c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.498Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importances\n",
    "importances = grid.best_estimator_.feature_importances_\n",
    "\n",
    "# Create a DataFrame\n",
    "importances_df = pd.DataFrame({'feature': X_train.columns, 'importance': importances})\n",
    "\n",
    "# Sort by importance\n",
    "importances_df = importances_df.sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "importances_df.plot.bar(x='feature', y='importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f12eb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.499Z"
    }
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Create a lime explainer object\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,\n",
    "                                                   feature_names=X_train.columns.values.tolist(), \n",
    "                                                   class_names=['Non-Default', 'Default'], \n",
    "                                                   verbose=True, \n",
    "                                                   mode='classification')\n",
    "\n",
    "# Pick the observation in the validation set for which explanation is required\n",
    "observation_1 = X_test.values[0]\n",
    "\n",
    "# Get the explanation for RandomForest\n",
    "exp = explainer.explain_instance(observation_1, grid.best_estimator_.predict_proba, num_features=5)\n",
    "\n",
    "exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ecd3d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.500Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "install shap\n",
    "import shap\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(grid.best_estimator_)\n",
    "\n",
    "# calculate shap values\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "263.505432px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
