{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1385a145",
   "metadata": {},
   "source": [
    "# Some Notes for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69016e",
   "metadata": {},
   "source": [
    "## graph-tool installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc488b27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T13:04:50.811346Z",
     "start_time": "2023-07-20T13:04:50.808128Z"
    }
   },
   "outputs": [],
   "source": [
    "#on windows: \n",
    "#use the windows key\n",
    "#type anaconda\n",
    "#open the anaconda power prompt as ADMINISTRATOR (right click). \n",
    "#conda activate base\n",
    "#conda create -n myenv python=3.8. \n",
    "#conda install -c conda-forge graph-tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be0d53",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adb1ba",
   "metadata": {},
   "source": [
    "## Basic file process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91b5a26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:07:16.020250Z",
     "start_time": "2023-07-25T17:07:16.006649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yil1/opt/anaconda3/envs/P2P/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check your python installation\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73307d27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:07:16.575397Z",
     "start_time": "2023-07-25T17:07:16.568960Z"
    }
   },
   "outputs": [],
   "source": [
    "def install_missing_packages(package_names):\n",
    "    \"\"\"\n",
    "    Install Missing Packages\n",
    "\n",
    "    This function checks if a list of packages is already installed and installs any missing packages using pip.\n",
    "\n",
    "    Parameters:\n",
    "    - package_names (list): A list of package names to be installed.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Note: This function requires the `subprocess` and `importlib` modules to be imported.\n",
    "\n",
    "    Example Usage:\n",
    "    install_missing_packages(['h2o', 'numpy', 'pandas'])\n",
    "    \"\"\"\n",
    "    import importlib\n",
    "    import subprocess\n",
    "\n",
    "\n",
    "    for package_name in package_names:\n",
    "        try:\n",
    "            importlib.import_module(package_name)\n",
    "            print(f\"{package_name} package is already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"{package_name} package not found, installing with pip...\")\n",
    "            subprocess.call(['pip', 'install', package_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "818fc30d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:07:19.754119Z",
     "start_time": "2023-07-25T17:07:18.203445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "networkx package is already installed\n",
      "gower package is already installed\n",
      "statsmodels package is already installed\n",
      "pyarrow package is already installed\n",
      "seaborn package is already installed\n",
      "lime package is already installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _reverse_window(order, start, length):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/links.py:5: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def identity(x):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/links.py:10: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _identity_inverse(x):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/links.py:15: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def logit(x):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/links.py:20: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _logit_inverse(x):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shap package is already installed\n"
     ]
    }
   ],
   "source": [
    "package_list = [\"networkx\", \"gower\",\"statsmodels\",\"pyarrow\",\"seaborn\", \"lime\", \"shap\"]\n",
    "install_missing_packages(package_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f6c989c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:07:28.877005Z",
     "start_time": "2023-07-25T17:07:28.837859Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "clean_data(df):\n",
    "This code defines a function called `clean_data`, which takes a DataFrame (usually a DataFrame object in the pandas library) as input and performs a series of data cleaning operations on it. The specific cleaning steps are as follows:\n",
    "\n",
    "1. **Delete missing values**: Use the `dropna()` method to remove rows containing missing values.\n",
    "\n",
    "2. **Filter specific rows**: keep the rows whose `lang.1` column is equal to 1, and reset the index.\n",
    "\n",
    "3. **Delete 'lang' column**: Delete all columns containing \"lang\".\n",
    "\n",
    "4. **Delete Date Columns**: Delete `date.start` and `date.end` columns as they are considered irrelevant.\n",
    "\n",
    "5. **Remove Look-Bias Variables**: Define a list containing look-ahead-bias variables and use the `drop()` method to drop those columns from the DataFrame.\n",
    "\n",
    "6. **Delete Duplicate Income Variables**: Delete all columns that contain \"inc.\" and do not contain \".no\".\n",
    "\n",
    "7. **Delete Dummy Variables**: Defines a list of dummy variables to delete and removes these columns from the DataFrame.\n",
    "\n",
    "8. **Generate Correlation Matrix**: Use the `corr()` method to generate the correlation matrix of DataFrame.\n",
    "\n",
    "9. **Choose the upper triangle of the correlation matrix**: To avoid multicollinearity, select the upper triangle of the correlation matrix.\n",
    "\n",
    "10. **Find High Correlation Columns**: Find columns that have a correlation higher than 0.95 with other columns.\n",
    "\n",
    "11. **Delete High Correlation Columns**: Delete the high correlation columns found in step 10.\n",
    "\n",
    "Finally, the function returns the cleaned DataFrame.\n",
    "\n",
    "The purpose of this code is to prepare the data for subsequent analysis or modeling. It cleans data by removing missing values, filtering specific rows, removing irrelevant or biased columns, and handling multicollinearity.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import random\n",
    "\n",
    "# Define a function to calculate variance inflation factor (VIF) for all variables in a given DataFrame.\n",
    "def calculate_vif(df):\n",
    "    \"\"\"Calculates variance inflation factor for all columns in df. It should contain \n",
    "    only exogeneous variables.\"\"\"\n",
    "    \n",
    "    # Create an empty DataFrame\n",
    "    vif = pd.DataFrame()\n",
    "    \n",
    "    # Calculate VIF for every column (variable) in the DataFrame\n",
    "    vif[\"VIF Factor\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    \n",
    "    # Store column names\n",
    "    vif[\"features\"] = df.columns\n",
    "    \n",
    "    # Sort by VIF Factor in descending order\n",
    "    vif = vif.sort_values(\"VIF Factor\", ascending=False)\n",
    "    return vif\n",
    "\n",
    "# Define a function to clean the DataFrame, removing irrelevant and problematic features.\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Clean the given DataFrame, drop unnecessary columns, handle missing data, remove biased variables, \n",
    "    avoid multicollinearity by checking correlation, and keep only relevant columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Filter for only rows where 'lang.1' equals 1, and drop 'lang' columns\n",
    "    df = df[df[\"lang.1\"] == 1].reset_index(drop=True)\n",
    "    df = df.drop([x for x in df.columns if \"lang\" in x], axis=1)\n",
    "\n",
    "    # Drop date columns as they are not considered relevant\n",
    "    df = df.drop([\"date.start\", \"date.end\"], axis=1)\n",
    "\n",
    "    # List of forward-looking biased variables\n",
    "    fwl_bias = [\n",
    "        \"return\",\n",
    "        \"RR1\",\n",
    "        \"RR2.Mean\",\n",
    "        \"RR2.Median\",\n",
    "        \"RR2.WMean\",\n",
    "        \"NPRP\",\n",
    "        \"NPRA\",\n",
    "        \"FVCI\",\n",
    "        \"FVCI.Mean\",\n",
    "        \"FVCI.Median\",\n",
    "        \"FVCI.WMean\",\n",
    "    ]\n",
    "    # Drop these forward-looking biased variables\n",
    "    df = df.drop(fwl_bias, axis=1)\n",
    "\n",
    "    # Drop duplicate income variables, keep only those with '.no' in name\n",
    "    df = df.drop([x for x in df.columns if \"inc.\" in x and \".no\" in x], axis=1)\n",
    "\n",
    "    # List of dummy variables to drop\n",
    "    dummies_to_drop = [\"AA\", \"educ.6\", \"em.dur.5p\", \"use.m\", \"ver.2\", \"Mining\", \"Utilities\"]\n",
    "    df = df.drop(dummies_to_drop, axis=1)\n",
    "\n",
    "    # Generate a correlation matrix of the DataFrame\n",
    "    corr_matrix = df.corr().abs()\n",
    "    \n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Find columns with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    \n",
    "    # Drop these columns from the DataFrame\n",
    "    df = df.drop(df[to_drop], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_balanced_sample(df, n, replace = False):\n",
    "    \"\"\"Create a balanced sample of size 2n from df. Ensures that the sample contains an equal number of instances for each class.\"\"\"\n",
    "    \n",
    "    # Draw a random sample of size 'n' from the non-default class (default = 0) and from the default class (default = 1)\n",
    "    # replace=True allows for resampling\n",
    "    # Random state ensures reproducibility\n",
    "    df_sample = pd.concat(\n",
    "        [\n",
    "            df[df[\"default\"] == 0].sample(n=n, random_state=1, replace=replace),\n",
    "            df[df[\"default\"] == 1].sample(n=n, random_state=1, replace=replace),\n",
    "        ]\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    return df_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "010f5859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:07:29.590696Z",
     "start_time": "2023-07-25T17:07:29.579290Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def generate_file_path(file_name):\n",
    "    \"\"\"\n",
    "    Generate a file path for a file located one directory level up.\n",
    "\n",
    "    Parameters:\n",
    "    file_name (str): The name of the file including any subdirectories from the parent directory.\n",
    "\n",
    "    Returns:\n",
    "    file_path (str): The full path to the file.\n",
    "    \"\"\"\n",
    "    # Get the current working directory\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # Get the parent directory\n",
    "    parent_dir = os.path.dirname(cwd)\n",
    "\n",
    "    # Define the file path by joining the parent directory path with the file name\n",
    "    file_path = os.path.join(parent_dir, file_name)\n",
    "\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb1c610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:08:07.414831Z",
     "start_time": "2023-07-25T17:08:07.407314Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to save a pandas dataframe as csv to disc.\n",
    "def save_df_to_csv(df, file_path):\n",
    "    \"\"\"\n",
    "    This function saves a pandas DataFrame to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to save.\n",
    "    file_path (str): The file path where to save the DataFrame, including the filename.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    df.to_csv(file_path, index=False)  # Set index=False to not save row indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8116cf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:08:07.832681Z",
     "start_time": "2023-07-25T17:08:07.824498Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_summary_stats(df, cols):\n",
    "    \"\"\"\n",
    "    This function computes summary statistics for specified columns in a pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The dataframe on which to compute summary statistics.\n",
    "    cols (list): A list of column names for which to compute summary statistics.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A dataframe with summary statistics for the specified columns.\n",
    "\n",
    "    Example:\n",
    "    summary_stats = compute_summary_stats(df, ['liab.I', 'inc.total', 'MonthlyPayment', 'log.amount', 'time', 'Interest', 'AmountOfPreviousLoansBeforeLoan', 'NoOfPreviousLoansBeforeLoan', 'Age'])\n",
    "    print(summary_stats)\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if all columns exist in dataframe\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            print(f'Column {col} does not exist in the dataframe.')\n",
    "            return None\n",
    "\n",
    "    # Compute summary statistics\n",
    "    summary_stats = df[cols].describe()\n",
    "\n",
    "    return summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72b26031",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:08:08.434440Z",
     "start_time": "2023-07-25T17:08:08.425769Z"
    }
   },
   "outputs": [],
   "source": [
    "def df_to_latex(df):\n",
    "    \"\"\"\n",
    "    This function converts a pandas DataFrame into a LaTeX table.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The dataframe to convert to LaTeX.\n",
    "\n",
    "    Returns:\n",
    "    str: A string of LaTeX code for a table with the data from the DataFrame.\n",
    "\n",
    "    Example:\n",
    "    latex_code = df_to_latex(summary_stats)\n",
    "    print(latex_code)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert DataFrame to LaTeX\n",
    "    latex_code = df.to_latex()\n",
    "\n",
    "    return latex_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82950439",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:08:09.917883Z",
     "start_time": "2023-07-25T17:08:09.908711Z"
    }
   },
   "outputs": [],
   "source": [
    "# retrieving the number of non-defaulted vs. defaulted loans\n",
    "def count_defaults(df):\n",
    "    \"\"\"\n",
    "    This function counts and prints the number of defaulted loans in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the loan data. It must contain a 'default' column \n",
    "                    with binary values: 1 for default and 0 otherwise.\n",
    "\n",
    "    Returns:\n",
    "    None. The function directly prints the number of defaulted loans.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_defaults = df['default'].sum()\n",
    "    print(f\"The number of loans that have defaulted is: {num_defaults}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2936d98",
   "metadata": {},
   "source": [
    "## Gowers distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19655a19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:08:10.819134Z",
     "start_time": "2023-07-25T17:08:10.804891Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gower\n",
    "\n",
    "\n",
    "def calculate_and_save_gowers_distance(dataframe, output_file_path):\n",
    "    \"\"\"\n",
    "    Calculates Gower's distance matrix for a given DataFrame and saves the result as a numpy array.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pandas.DataFrame): The input DataFrame.\n",
    "    output_file_path (str): The path where the resulting numpy array should be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Drop the 'default' column to keep only features\n",
    "        dataframe = dataframe.drop([\"default\"], axis=1)\n",
    "\n",
    "        # Identify dummy columns (faster for gower calculations)\n",
    "        dummy_columns = [\n",
    "            column for column in dataframe.columns\n",
    "            if ((dataframe[column] == 0) | (dataframe[column] == 1)).all()\n",
    "        ]\n",
    "        categorical_variables = [\n",
    "            column in dummy_columns for column in dataframe.columns\n",
    "        ]\n",
    "\n",
    "        # Calculate Gower's distance\n",
    "        distance_matrix = gower.gower_matrix(\n",
    "            dataframe, cat_features=categorical_variables)\n",
    "\n",
    "        # Save the distance matrix as a numpy array\n",
    "        np.save(output_file_path, distance_matrix)\n",
    "\n",
    "        print(f\"Distance matrix saved successfully at {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return distance_matrix\n",
    "\n",
    "\n",
    "# Changes made:\n",
    "\n",
    "# 1. Renamed the function to `calculate_and_save_gowers_distance` for clarity.\n",
    "# 2. Added a try-except block to handle potential errors during execution.\n",
    "# 3. Added comments to explain each step of the process.\n",
    "# 4. Improved the print statement to include the output file path for better tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "817f5f30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:08:11.699972Z",
     "start_time": "2023-07-25T17:08:11.690544Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_gowers_distance_matrix(file_path):\n",
    "    \"\"\"\n",
    "    Loads Gower's distance matrix from a given file path.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path from where the numpy array should be loaded.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Loaded Gower's distance matrix.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        distance_matrix = np.load(file_path)\n",
    "        print(f\"Distance matrix loaded successfully from {file_path}\")\n",
    "        return distance_matrix\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52260458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:08:13.563359Z",
     "start_time": "2023-07-25T17:08:12.408402Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from graph_tool.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e7761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:08:13.588060Z",
     "start_time": "2023-07-25T17:08:13.588054Z"
    }
   },
   "outputs": [],
   "source": [
    "def matrix_to_graph_tool(adj):\n",
    "    \"\"\"\n",
    "    Convert adjacency matrix to graph using graph-tool.\n",
    "\n",
    "    Parameters:\n",
    "    adj (numpy.ndarray): The adjacency matrix.\n",
    "\n",
    "    Returns:\n",
    "    g (graph_tool.Graph): The graph.\n",
    "    \"\"\"\n",
    "    # Extract index and weights from the adjacency matrix.\n",
    "    # np.nonzero(np.triu(adj, 1)) returns the indices of the upper triangle of the matrix,\n",
    "    # excluding the diagonal. This is because for an undirected graph, the adjacency matrix\n",
    "    # is symmetric, and we only need to consider half of the matrix to get all the edges.\n",
    "    idx = np.nonzero(np.triu(adj, 1))\n",
    "\n",
    "    # Get the weights of the edges from the adjacency matrix.\n",
    "    weights = adj[idx]\n",
    "\n",
    "    # Create an empty graph.\n",
    "    g = Graph()\n",
    "\n",
    "    # Add edges to the graph. np.transpose(idx) gives a 2D array where each row is the indices\n",
    "    # of the two vertices of an edge.\n",
    "    g.add_edge_list(np.transpose(idx))\n",
    "\n",
    "    # Create an edge property map for the weights of the edges.\n",
    "    edge_weight = g.new_edge_property(\"double\")\n",
    "\n",
    "    # Assign the weights to the edge property map.\n",
    "    edge_weight.a = weights\n",
    "\n",
    "    # Add the edge property map to the graph.\n",
    "    g.edge_properties[\"edge_weight\"] = edge_weight\n",
    "\n",
    "    return g\n",
    "\n",
    "def calculate_centrality_measures(g):\n",
    "    \"\"\"\n",
    "    Calculate various centrality measures for the graph.\n",
    "\n",
    "    Parameters:\n",
    "    g (graph_tool.Graph): The graph.\n",
    "\n",
    "    Returns:\n",
    "    df (pandas.DataFrame): DataFrame with centrality measures.\n",
    "    \"\"\"\n",
    "    # Calculate PageRank centrality. This measure reflects the importance of a node in the graph.\n",
    "    # Nodes with a high PageRank centrality are those that are well connected or connected to well-connected nodes.\n",
    "    pr_w = pagerank(g, weight=g.ep.edge_weight).a\n",
    "\n",
    "    # Calculate betweenness centrality. This measure reflects the amount of control that a node exerts over the interactions of other nodes in the graph.\n",
    "    # Nodes with high betweenness centrality are those that lie on many shortest paths between other nodes.\n",
    "    bw_w = betweenness(g, weight=g.ep.edge_weight)[0].a\n",
    "\n",
    "    # Calculate closeness centrality. This measure reflects how close a node is to all other nodes in the graph.\n",
    "    # Nodes with high closeness centrality can reach other nodes quickly.\n",
    "    cl_w = closeness(g, weight=g.ep.edge_weight).a\n",
    "\n",
    "    # Calculate eigenvector centrality. This measure reflects the influence of a node in a network.\n",
    "    # Nodes with high eigenvector centrality are those connected to many nodes who themselves have high centrality.\n",
    "    ev_w = eigenvector(g, weight=g.ep.edge_weight)[1].a\n",
    "\n",
    "    # Calculate Katz centrality. This measure is a generalization of degree centrality and eigenvector centrality.\n",
    "    kt_w = katz(g, weight=g.ep.edge_weight).a\n",
    "\n",
    "    # Calculate HITS authority and hub scores. The HITS algorithm computes two numbers for a node: \n",
    "    # Authorities estimates the node value based on the incoming links. \n",
    "    # Hubs estimates the node value based on outgoing links.\n",
    "    hits_aut_w = hits(g, weight=g.ep.edge_weight)[1].a\n",
    "    hits_hub_w = hits(g, weight=g.ep.edge_weight)[2].a\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"pagerank\": pr_w,\n",
    "        \"betweenness\": bw_w,\n",
    "        \"closeness\": cl_w,\n",
    "        \"eigenvector\": ev_w,\n",
    "        \"katz\": kt_w,\n",
    "        \"authority\": hits_aut_w,\n",
    "        \"hub\": hits_hub_w,\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fcba95",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eceb8b8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:18:51.603273Z",
     "start_time": "2023-07-25T17:18:51.590235Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def save_histogram_pdf(array, file_path, samplesPerBin):\n",
    "    \"\"\"\n",
    "    Saves a histogram of a numpy array to a given file path in PDF format.\n",
    "\n",
    "    Parameters:\n",
    "    array (numpy.ndarray): The input numpy array.\n",
    "    file_path (str): The path where the PDF should be saved.\n",
    "    samplesPerBin (int): The desired number of samples per bin.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate the number of bins\n",
    "        num_bins = array.size // samplesPerBin\n",
    "        if array.size % samplesPerBin > 0:  # if there's a remainder, add an extra bin\n",
    "            num_bins += 1\n",
    "\n",
    "        # Ensure we have at least 1 bin\n",
    "        num_bins = max(1, num_bins)\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        directory = os.path.dirname(file_path)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        plt.hist(array.flatten(), bins=num_bins)\n",
    "        plt.title('Histogram of ndarray values')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.savefig(file_path, format='pdf')\n",
    "        plt.close()\n",
    "        print(f\"Histogram saved successfully at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad4c31",
   "metadata": {},
   "source": [
    "# Read back distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81cccbb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:08:16.132198Z",
     "start_time": "2023-07-25T17:08:16.125649Z"
    }
   },
   "outputs": [],
   "source": [
    "num_default_samples=12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f68827f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:08:16.611062Z",
     "start_time": "2023-07-25T17:08:16.604097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yil1/p2p-model-bondora/data//Dist_matrix(24000).npy\n"
     ]
    }
   ],
   "source": [
    "dist_matrix_path = generate_file_path(\"data//Dist_matrix(\"+str(num_default_samples*2)+\").npy\")\n",
    "print(dist_matrix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76ef808b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:08:18.132898Z",
     "start_time": "2023-07-25T17:08:17.737583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance matrix loaded successfully from /Users/yil1/p2p-model-bondora/data//Dist_matrix(24000).npy\n"
     ]
    }
   ],
   "source": [
    "distance_matrix=load_gowers_distance_matrix(dist_matrix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56cf1fcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:08:20.062029Z",
     "start_time": "2023-07-25T17:08:20.053523Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(24000, 24000)\n"
     ]
    }
   ],
   "source": [
    "print(type(distance_matrix))\n",
    "print(distance_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4463cb51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:08:23.005921Z",
     "start_time": "2023-07-25T17:08:22.997244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yil1/p2p-model-bondora/graph/paper2/Dist_matrix(24000).pdf\n"
     ]
    }
   ],
   "source": [
    "dist_matrix_plot_path = generate_file_path(\"graph/paper2/Dist_matrix(\"+str(num_default_samples*2)+\").pdf\")\n",
    "print(dist_matrix_plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79b74941",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:19:51.908531Z",
     "start_time": "2023-07-25T17:19:24.526871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram saved successfully at /Users/yil1/p2p-model-bondora/graph/paper2/Dist_matrix(24000).pdf\n"
     ]
    }
   ],
   "source": [
    "save_histogram_pdf(array=distance_matrix,\n",
    "                   file_path=dist_matrix_plot_path,\n",
    "                   samplesPerBin=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a3b56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ada8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd34d251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9783e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0864c001",
   "metadata": {},
   "source": [
    "# Add centrality measures\n",
    "more on graph-tools.\n",
    "https://robert-haas.github.io/gravis-docs/code/examples/external_tools/graph-tool.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e516943",
   "metadata": {},
   "source": [
    "## Add centrality measures\n",
    "Wrapped the graph creation and centrality measure calculation into separate functions for better readability and reusability.\n",
    "Removed unnecessary list ls. Instead, created the DataFrame directly using a dictionary.\n",
    "Added docstrings to the functions to explain what they do.\n",
    "Removed the unused time import.\n",
    "Added comments to explain each step of the process.\n",
    "Used pd.concat() to merge the original DataFrame with the centrality measures DataFrame. This is more efficient and readable than creating a new DataFrame and then adding columns to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09154bc",
   "metadata": {},
   "source": [
    "In a weighted graph, the weight of an edge can represent various things depending on the context of the problem. It could represent the strength of the connection between the nodes, the distance between the nodes, the cost of moving from one node to another, etc.\n",
    "\n",
    "The way the weight of an edge affects the centrality measures depends on the specific measure:\n",
    "\n",
    "1. **PageRank**: In the context of PageRank, the weight of an edge can be interpreted as the probability of a random walk following the edge. A higher weight means that it's more likely for the random walk to follow the edge. Therefore, nodes connected by high-weight edges will have a higher PageRank centrality.\n",
    "\n",
    "2. **Betweenness**: In the context of betweenness centrality, the weight of an edge can be interpreted as the cost of traversing the edge. A lower weight means that it's less costly to traverse the edge. Therefore, nodes connected by low-weight edges will have a higher betweenness centrality.\n",
    "\n",
    "3. **Closeness**: In the context of closeness centrality, the weight of an edge can be interpreted as the distance between the nodes. A lower weight means that the nodes are closer to each other. Therefore, nodes connected by low-weight edges will have a higher closeness centrality.\n",
    "\n",
    "4. **Eigenvector**: In the context of eigenvector centrality, the weight of an edge contributes to the centrality of the nodes it connects. A higher weight means that the nodes will have a higher eigenvector centrality.\n",
    "\n",
    "5. **Katz**: In the context of Katz centrality, the weight of an edge contributes to the centrality of the nodes it connects. A higher weight means that the nodes will have a higher Katz centrality.\n",
    "\n",
    "6. **HITS**: In the context of HITS, the weight of an edge contributes to the authority and hub scores of the nodes it connects. A higher weight means that the nodes will have a higher authority and hub scores.\n",
    "\n",
    "In all these measures, the weight of an edge is used to quantify the importance of the edge in the graph. The exact interpretation of the weight depends on the context of the problem and the specific centrality measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f128ecc8",
   "metadata": {},
   "source": [
    "## Understand adjacency and distance matrix\n",
    "A Gower's distance matrix and an adjacency matrix are both square matrices that represent relationships between entities, but they are used in different contexts and convey different types of information.\n",
    "\n",
    "1. **Gower's Distance Matrix**: This matrix is used in the context of cluster analysis or hierarchical clustering. Each entry in the matrix represents the Gower's distance between two entities (e.g., individuals, observations, etc.). Gower's distance is a measure of dissimilarity between two entities and takes into account both numerical and categorical variables. The smaller the Gower's distance, the more similar the two entities are.\n",
    "\n",
    "2. **Adjacency Matrix**: This matrix is used in the context of graph theory. Each entry in the matrix indicates whether two vertices (i.e., nodes) in a graph are adjacent to each other. In an unweighted graph, the entries are binary (0 or 1), indicating the absence or presence of an edge between two vertices. In a weighted graph, the entries can be any non-negative real number, representing the weight of the edge between two vertices.\n",
    "\n",
    "In some cases, a Gower's distance matrix could be transformed into a form of adjacency matrix. For example, you might create a graph where each entity is a node, and an edge exists between two nodes if their Gower's distance is below a certain threshold. The resulting adjacency matrix would represent a graph of entities that are similar to each other according to the Gower's distance. However, this would be a specific use case and not a general relationship between Gower's distance matrices and adjacency matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e9d63",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "1. **Adjacency Matrix**: An adjacency matrix is a square matrix used to represent a finite graph. In an unweighted graph, the elements of the adjacency matrix (A[i][j]) indicate whether pairs of vertices are adjacent or not in the graph -- 1 for adjacent, 0 for not adjacent. In a weighted graph, the elements of the adjacency matrix represent the weights of the edges.\n",
    "\n",
    "2. **Distance Matrix**: A distance matrix is a square matrix containing the distances, taken pairwise, between the elements of a set. In an unweighted graph, the elements of the distance matrix (D[i][j]) represent the number of edges in the shortest path between each pair of vertices. In a weighted graph, the elements of the distance matrix represent the sum of the weights of the shortest path between each pair of vertices.\n",
    "\n",
    "3. **Gower's Distance Matrix**: Gower's distance matrix is a special type of distance matrix primarily used for mixed data types. It is not commonly used in graph theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ce5c011",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:07:24.709427Z",
     "start_time": "2023-07-20T14:07:23.990827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load adjacency matrix\n",
    "# The adjacency matrix is a square matrix used to represent a finite graph.\n",
    "# The elements of the matrix indicate whether pairs of vertices are adjacent or not in the graph.\n",
    "adj_path = generate_file_path(\"data//Dist_matrix(\"+str(num_default_samples*2)+\").npy\")\n",
    "adj = np.load(adj_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a05fad",
   "metadata": {},
   "source": [
    "Explain: Here, this graph is a fully connected, which means each pair of nodes has an edge. Thus, the Adjacency Matrix is equivalent to the Gower's Distance Matrix.\n",
    "\n",
    "(When a graph is not fully connected, these two matrix are not equivalent any more.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b0d4d5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:07:47.744378Z",
     "start_time": "2023-07-20T14:07:30.433281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert adjacency matrix to graph\n",
    "# The adjacency matrix is then converted into a graph object using the `matrix_to_graph_tool` function.\n",
    "# This function takes the adjacency matrix as input and returns a graph object that can be manipulated using the `graph-tool` library.\n",
    "g = matrix_to_graph_tool(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048b53de",
   "metadata": {},
   "source": [
    "Attention here! We can use alpha threathold here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a808b8e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:14:52.561149Z",
     "start_time": "2023-07-20T14:07:53.800246Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate minimum spanning tree and set as edge filter\n",
    "# A minimum spanning tree (MST) of a graph is a subset of the edges of the graph that connects all the vertices together,\n",
    "# without any cycles and with the minimum possible total edge weight.\n",
    "# The `min_spanning_tree` function from the `graph-tool` library is used to calculate the MST.\n",
    "# The resulting MST is then set as an edge filter on the graph using `g.set_edge_filter(tree)`.\n",
    "# This means that only the edges that are part of the MST will be considered in subsequent operations on the graph.\n",
    "tree = min_spanning_tree(g, weights=g.ep.edge_weight)\n",
    "g.set_edge_filter(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cd2d11d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:15:01.757318Z",
     "start_time": "2023-07-20T14:15:00.739855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph object, directed, with 24000 vertices and 23999 edges, 1 internal edge property, edges filtered by (<EdgePropertyMap object with value type 'bool', for Graph 0x2ab455e80, at 0x17ff403a0>, False), vertices filtered by (<VertexPropertyMap object with value type 'bool', for Graph 0x2ab455e80, at 0x17ab72c10>, False), at 0x2ab455e80>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a447ea8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:16:05.608317Z",
     "start_time": "2023-07-20T14:15:04.163030Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate centrality measures\n",
    "# Finally, several centrality measures are calculated for the graph using the `calculate_centrality_measures` function.\n",
    "# Centrality measures provide a way of identifying the most important vertices within a graph.\n",
    "# They are commonly used in network analysis to determine the relative importance of a vertex within the graph.\n",
    "# The resulting centrality measures are stored in a DataFrame.\n",
    "centrality_df = calculate_centrality_measures(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2e5b39b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:22:58.428546Z",
     "start_time": "2023-07-20T14:22:58.410359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pagerank</th>\n",
       "      <th>betweenness</th>\n",
       "      <th>closeness</th>\n",
       "      <th>eigenvector</th>\n",
       "      <th>katz</th>\n",
       "      <th>authority</th>\n",
       "      <th>hub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.902073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006453</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.078653e-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.976149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006453</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.410830e-83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.396533</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006453</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.701703e-47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.359868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006453</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.883548e-67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.029100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006453</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.762067e-95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006456</td>\n",
       "      <td>6.289251e-103</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>3.161870e-142</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006455</td>\n",
       "      <td>5.968064e-126</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>4.234108e-49</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006456</td>\n",
       "      <td>1.081868e-80</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pagerank  betweenness  closeness  eigenvector      katz      authority  \\\n",
       "0      0.000023          0.0   8.902073          NaN  0.006453   0.000000e+00   \n",
       "1      0.000023          0.0  15.976149          NaN  0.006453   0.000000e+00   \n",
       "2      0.000023          0.0  13.396533          NaN  0.006453   0.000000e+00   \n",
       "3      0.000023          0.0   7.359868          NaN  0.006453   0.000000e+00   \n",
       "4      0.000023          0.0  26.029100          NaN  0.006453   0.000000e+00   \n",
       "...         ...          ...        ...          ...       ...            ...   \n",
       "23995  0.000127          0.0        NaN          NaN  0.006456  6.289251e-103   \n",
       "23996  0.000078          0.0        NaN          NaN  0.006454  3.161870e-142   \n",
       "23997  0.000184          0.0        NaN          NaN  0.006455  5.968064e-126   \n",
       "23998  0.000034          0.0        NaN          NaN  0.006454   4.234108e-49   \n",
       "23999  0.000131          0.0        NaN          NaN  0.006456   1.081868e-80   \n",
       "\n",
       "                hub  \n",
       "0      2.078653e-22  \n",
       "1      2.410830e-83  \n",
       "2      6.701703e-47  \n",
       "3      3.883548e-67  \n",
       "4      1.762067e-95  \n",
       "...             ...  \n",
       "23995  0.000000e+00  \n",
       "23996  0.000000e+00  \n",
       "23997  0.000000e+00  \n",
       "23998  0.000000e+00  \n",
       "23999  0.000000e+00  \n",
       "\n",
       "[24000 rows x 7 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centrality_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "489a1c87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:23:02.567975Z",
     "start_time": "2023-07-20T14:23:02.562956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(centrality_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc739f5f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.447Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def plot_histogram(df, column):\n",
    "    \"\"\"\n",
    "    \n",
    "    NaN\n",
    "    \"\"\"\n",
    "    if df[column].isna().all(): # NaN\n",
    "        print(f\"Warning: Column {column} only contains NaN values. Skipping...\")\n",
    "        return\n",
    "\n",
    "    plt.figure()\n",
    "    sns.histplot(df[column], kde=False, bins=50) # bins50\n",
    "    plt.title(f\"Histogram of {column}\")\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "centrality_data_path = generate_file_path(\"graph/Bondora_centrality_sample(\"+str(num_default_samples*2)+\").pdf\")\n",
    "\n",
    "with PdfPages(centrality_data_path) as pdf: # PDF\n",
    "    for column in centrality_df.columns:\n",
    "        plot_histogram(centrality_df, column)\n",
    "        if plt.get_fignums():  # figure\n",
    "            pdf.savefig()  # figurepdf\n",
    "            plt.close()    # figure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971cd0c1",
   "metadata": {},
   "source": [
    "## Combine ans save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3af8b54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:23:13.182339Z",
     "start_time": "2023-07-20T14:23:13.162815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yil1/p2p-model-bondora/data/Bondora_sample(24000).feather\n"
     ]
    }
   ],
   "source": [
    "# Load original dataset\n",
    "sample_data_path = generate_file_path(\"data/Bondora_sample(\"+str(num_default_samples*2)+\").feather\")\n",
    "df_sample = pd.read_feather(sample_data_path)\n",
    "print(sample_data_path)\n",
    "\n",
    "# Merge with original dataset\n",
    "df_sample_with_centrality = pd.concat([df_sample, centrality_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8311e1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:23:20.200838Z",
     "start_time": "2023-07-20T14:23:19.452086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yil1/p2p-model-bondora/data//Bondora_sample(24000)_with_centrality.csv\n"
     ]
    }
   ],
   "source": [
    "df = df_sample_with_centrality\n",
    "\n",
    "# Define the text file path\n",
    "txt_file_path = generate_file_path(\"data//Bondora_sample(\"+str(num_default_samples*2)+\")_with_centrality.csv\")\n",
    "print(txt_file_path)\n",
    "# Write the DataFrame to the text file\n",
    "df.to_csv(txt_file_path, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07090795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "263.505432px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
